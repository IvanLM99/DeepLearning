{"cells":[{"cell_type":"markdown","metadata":{"id":"PUu2VSg0UO91"},"source":["<img src=\"https://drive.google.com/uc?export=view&id=1x-QAgitB-S5rxGGDqxsJ299ZQTfYtOhb\" width=180, align=\"center\"/>\n","\n","Master's degree in Intelligent Systems\n","\n","Subject: 11754 - Deep Learning\n","\n","Year: 2023-2024\n","\n","Professor: Miguel Ángel Calafat Torrens"],"id":"PUu2VSg0UO91"},{"cell_type":"markdown","metadata":{"id":"auburn-therapy"},"source":["# LAB 5 - Generative Adversarial Networks\n","\n","**In this lab you have to deliver only this file.**\n","\n","As you will see this notebook shows essentially the same cells that `LSS5-GAN.ipynb`, or at least, the important ones related to the WGAN-GP, without the chatter.\n","\n","You have to modify this file in the cells you want to achieve the following requirements:\n","* You have to define the generator and the critic with at least 5 blocks of convolutions.\n","* The latent space (input) of the generator must be of size 200 instead of 100.\n","* You have to perform a training for at least 40 epochs with the dataset (you can use the small version, or another one bigger, or even the full version, as you wish).\n","* You have to save checkpoints at two different epochs (for example, if you've trained 40 epochs, you could save checkpoints at epochs 20 and 40)\n","* You have to load the checkpoints of these two epochs and show a random generation of a batch of 25 images (for each checkpoint).\n","\n","**Remember that you have a helper functions in `helper_PR5.py` to load checkpoints and to show batches of tensors**, so you don't have to invent anything but the models.\n","\n","**You can modify, add or remove any cell that you want to fulfill the requirements.**\n"],"id":"auburn-therapy"},{"cell_type":"markdown","metadata":{"id":"4mUGsd-ysGs1"},"source":["# WGAN-GP"],"id":"4mUGsd-ysGs1"},{"cell_type":"markdown","metadata":{"id":"QHdYAzcHm_y-"},"source":["## Setting up\n","\n","In this notebook we will use the celebA dataset. You can find it in a lot of places all over the internet, but one good place to start is its own [website](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) where you scroll down to the \"**Download**\" section, select the link \"**In the wild images**\" go into the folder \"**Img**\" and download the file \"**image_align_celeba.zip**\". Maybe it's even easier download it from this [link of Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset) if you have an account (the account is free).\n","\n","Anyway, you just remember that **the CelebA dataset is available for non-commercial research purposes only**."],"id":"QHdYAzcHm_y-"},{"cell_type":"code","execution_count":null,"metadata":{"id":"gP0_MIRNnQqY"},"outputs":[],"source":["# Connect to your drive\n","from google.colab import drive, files\n","drive.mount('/content/gdrive')\n","%cd '/content/gdrive/MyDrive/LABS2024/LAB5'\n","%ls -l\n","\n","# Here the path of the project folder (which is where this file is) is inserted\n","# into the python path.\n","import pathlib\n","import sys\n","import os\n","import helper_PR5 as hp\n","\n","PROJECT_DIR = str(pathlib.Path().resolve())\n","sys.path.append(PROJECT_DIR)"],"id":"gP0_MIRNnQqY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"NH1OBO-Gz-CH"},"outputs":[],"source":["# Ensure you write down the correct path to your zip file with the dataset\n","dataset_zip_fullpath = '/content/gdrive/MyDrive/datasets/img_align_celeba_small.zip'"],"id":"NH1OBO-Gz-CH"},{"cell_type":"markdown","metadata":{"id":"H1mizjwJg0i9"},"source":["**Before running the next cell, make sure you have left the zip file in the location you want and assign the variable accordingly.**"],"id":"H1mizjwJg0i9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOSH1KeJz-Jq"},"outputs":[],"source":["# This cell will take several minutes the first time you run it.\n","\n","# You can execute this cell every time you run the code. It won't unzip\n","# anything if the dataset is already unzipped.\n","DATA_DIR = hp.extract_dataset(dataset_zip_fullpath,\n","                              remove_zip=True)"],"id":"iOSH1KeJz-Jq"},{"cell_type":"markdown","metadata":{"id":"TLGNkY3Sl-ic"},"source":["Perhaps, the first time you run this notebook, you might prefer using the CPU environment, because you'll need some time to understand what is being done (remember the GPU usage limitations set by Colab). Moreover, the GPU will only be necessary when you wish to execute the trainings, that is, when you know how the entire set works."],"id":"TLGNkY3Sl-ic"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDFNPJb34Qkg"},"outputs":[],"source":["# Import the necessary libraries\n","import PIL\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","\n","SEED = 42\n","torch.manual_seed(SEED)\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"id":"dDFNPJb34Qkg"},{"cell_type":"markdown","metadata":{"id":"IDAWZsI9ryjX"},"source":["## The dataset"],"id":"IDAWZsI9ryjX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGGx2Q0UiMki"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, img_folder, lim=2000, transforms=None):\n","        # lim is the max number of images that we want to use. It's default is\n","        # used when doing a prove of concept, so that we don't want to use all\n","        # images of the dataset\n","        self.img_folder = img_folder\n","        self.lim = lim\n","\n","        # Initialize empty lists for items and labels\n","        self.items = []\n","        self.labels = []\n","\n","        # Walk through all files in img_folder and its subfolders\n","        for root, _, files in os.walk(img_folder):\n","            for file in files:\n","                # Check if the file is an image\n","                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n","                    full_path = os.path.join(root, file)\n","                    self.items.append(full_path)\n","                    self.labels.append(file)\n","\n","                    # Stop if the limit is reached\n","                    if lim > 0 and len(self.items) >= lim:\n","                        break\n","\n","            if lim > 0 and len(self.items) >= lim:\n","                break\n","\n","        # Define the transformation pipeline\n","        self.transforms = transforms\n","\n","    def __getitem__(self, idx):\n","        # Load the image\n","        data = PIL.Image.open(self.items[idx]).convert('RGB')\n","\n","        # Apply the transformations if provided\n","        if self.transforms:\n","            data = self.transforms(data)\n","\n","        # Return the processed data and its corresponding label\n","        return data, self.labels[idx]\n","\n","    def __len__(self):\n","        return len(self.items)"],"id":"JGGx2Q0UiMki"},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8ITu66ALYQu"},"outputs":[],"source":["# Define the transformations to be done\n","class ToScaledTensor(transforms.ToTensor):\n","    def __init__(self, low=-1, high=1):\n","        super().__init__()\n","        self.low = low\n","        self.high = high\n","\n","    def __call__(self, img):\n","        # Convert image to tensor (same as ToTensor)\n","        tensor = super().__call__(img)\n","        # Scale the tensor to the desired range\n","        tensor = tensor * (self.high - self.low) + self.low\n","        return tensor"],"id":"k8ITu66ALYQu"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9WP9IA3qkfLS"},"outputs":[],"source":["# Size of the images (height and width)\n","img_size = 64\n","\n","# Transformations\n","transform = transforms.Compose([\n","    # Center crop the images so that they become square\n","    transforms.CenterCrop((178, 178)),\n","    # Resize the image to the specified size (h, w)\n","    transforms.Resize((img_size, img_size)),\n","    # Convert the image to a PyTorch tensor and scale to [-1, 1]\n","    ToScaledTensor(),\n","    ])"],"id":"9WP9IA3qkfLS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmeobSLCqrby"},"outputs":[],"source":["# limit is the max number of images to use (-1 if all)\n","\n","# Maybe you want to start working with fewer images to see how it works\n","limit = -1\n","\n","# Define the custom dataset object.\n","dataset = CustomDataset(DATA_DIR,\n","                        transforms=transform,\n","                        lim=limit)"],"id":"tmeobSLCqrby"},{"cell_type":"markdown","metadata":{"id":"wgkCh2Usfx40"},"source":["## The dataloader"],"id":"wgkCh2Usfx40"},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdlikL51Ctoy"},"outputs":[],"source":["# Batch size (number of images per batch)\n","batch_size = 128\n","\n","# Define the DataLoader\n","dataloader = DataLoader(dataset,\n","                        batch_size=batch_size,\n","                        shuffle=True)"],"id":"XdlikL51Ctoy"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQbnw98PAZQ4"},"outputs":[],"source":["# And now let's see images of the first batch\n","hp.show(next(iter(dataloader))[0])"],"id":"QQbnw98PAZQ4"},{"cell_type":"markdown","metadata":{"id":"mj_oCYEGIscL"},"source":["## The networks"],"id":"mj_oCYEGIscL"},{"cell_type":"markdown","source":["### The Critic"],"metadata":{"id":"0B8urEN8sJfW"},"id":"0B8urEN8sJfW"},{"cell_type":"code","execution_count":null,"metadata":{"id":"kq3t7N34z-VY"},"outputs":[],"source":["# Here define your version of the Critic\n","class Critic(nn.Module):\n","\n","    pass"],"id":"kq3t7N34z-VY"},{"cell_type":"markdown","source":["### The Generator"],"metadata":{"id":"X5LG1F-zrxZp"},"id":"X5LG1F-zrxZp"},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpdqlRz_4CU4"},"outputs":[],"source":["# Latent space size of the generator\n","z_dim = 200"],"id":"gpdqlRz_4CU4"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wf-Wv0Lt91Br"},"outputs":[],"source":["# Here define your version of the Generator\n","class Generator(nn.Module):\n","\n","    pass"],"id":"Wf-Wv0Lt91Br"},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPeo51DI91H3"},"outputs":[],"source":["# Instantiate the models. Of course you can initialize them with\n","# default values or not.\n","C = Critic()\n","G = Generator()"],"id":"UPeo51DI91H3"},{"cell_type":"markdown","metadata":{"id":"KvXzuZuJPJb0"},"source":["## Optimizers"],"id":"KvXzuZuJPJb0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRLlIETn8ds1"},"outputs":[],"source":["# Critic optimizer\n","\n","# Parameters\n","lr_c = 0.0002\n","beta1 = 0.5\n","beta2 = 0.9\n","\n","# Create optimizer for the critic C\n","c_optimizer = torch.optim.Adam(C.parameters(), lr_c, [beta1, beta2])"],"id":"JRLlIETn8ds1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dLrf3Ywn8dzV"},"outputs":[],"source":["# Generator optimizer\n","\n","# Parameters\n","lr_g = 0.0002\n","beta1 = 0.5\n","beta2 = 0.9\n","\n","# Create optimizers for generator G\n","g_optimizer = torch.optim.Adam(G.parameters(), lr_g, [beta1, beta2])"],"id":"dLrf3Ywn8dzV"},{"cell_type":"markdown","metadata":{"id":"zzTb7d5HYvtD"},"source":["## Losses"],"id":"zzTb7d5HYvtD"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHtFobwfLvvh"},"outputs":[],"source":["def Wasserstein_loss_fcn(input_tensor, **kwargs):\n","\n","    # Extract label_type from **kwargs with defaults\n","    label_type = kwargs.get(\"label_type\", \"real\")\n","\n","    # Ensure label_type is valid (real or fake)\n","    label_type = \"real\" if label_type.lower() not in (\"real\", \"fake\") \\\n","        else label_type.lower()\n","\n","    # Get sign of the losses\n","    sign = -1 if label_type == 'real' else 1\n","\n","    return input_tensor.mean() * sign"],"id":"YHtFobwfLvvh"},{"cell_type":"code","execution_count":null,"metadata":{"id":"hSul2XO1L0xa"},"outputs":[],"source":["def penalty_fcn(real_img, fake_img, critic, gamma=10):\n","    \"\"\"\n","    Calculate gradient penalty\n","    \"\"\"\n","\n","    # Interpolation between real and fake images\n","\n","    # Define alpha with shape bs x 1 x 1 x 1\n","    alpha = torch.rand(real_img.shape[0], 1, 1, 1,\n","                       device=DEVICE,\n","                       requires_grad=True)\n","\n","    # Calculate interpolated images\n","    # You can do it this way, that is easier to read, or with torch.lerp, that\n","    # is faster: `mix_images = real_img * alpha + fake_img * (1 - alpha)`\n","    mix_images = torch.lerp(fake_img, real_img, alpha)\n","\n","    # Get the critic prediction for the mixed images\n","    mix_pred = critic(mix_images)\n","\n","    # Calculate the gradients of the output with respect to the input.\n","    # I.e. the gradients of the prediction with respect to the mixed images.\n","    gradients = torch.autograd.grad(\n","        inputs=mix_images,\n","        outputs=mix_pred,\n","        grad_outputs=torch.ones_like(mix_pred),\n","        retain_graph=True,\n","        create_graph=True,\n","        only_inputs=True)[0]\n","\n","    # Reshape gradients to calculate norm\n","    gradients = gradients.view(len(gradients), -1)  # Flatten the gradients\n","\n","    # Use torch.norm to calculate the L2 norm of the gradients\n","    gradient_norm = torch.norm(gradients, p=2, dim=1)\n","\n","    # Calculate the gradient penalty\n","    gp = ((gradient_norm - 1) ** 2).mean()\n","\n","    return gp * gamma"],"id":"hSul2XO1L0xa"},{"cell_type":"markdown","metadata":{"id":"1kgcTmtHyrRC"},"source":["## Checkpoints"],"id":"1kgcTmtHyrRC"},{"cell_type":"markdown","metadata":{"id":"rW0fGckzSa40"},"source":["Check the criteria used to perform or not the checkpoint."],"id":"rW0fGckzSa40"},{"cell_type":"code","execution_count":null,"metadata":{"id":"JlPnBVMhyrpQ"},"outputs":[],"source":["def checkpointer(epoch,\n","                 epoch_gener_loss,\n","                 best_gen_loss,\n","                 config,\n","                 save_step,\n","                 starting_from=20):\n","    \"\"\"\n","    Do checkpoints if required.\n","\n","    Evaluates the performance of the generator at the current epoch and decides\n","    whether to save a checkpoint. Checkpoints are saved if the generator's loss\n","    is improved past a specified epoch or at regular intervals defined by the\n","    save_step parameter.\n","\n","    The function logs the reason for saving a checkpoint, updates the best\n","    generator loss if necessary, and calls a separate function to actually save\n","    the checkpoint.\n","\n","    Parameters:\n","    - epoch (int): The current training epoch.\n","    - epoch_gener_loss (float): The generator's loss for the current epoch.\n","    - best_gen_loss (float): The best generator loss observed so far in the\n","        training.\n","    - config (dict): A dictionary containing the configuration of the model,\n","        including the model itself and its optimizer.\n","    - save_step (int): The interval of epochs at which checkpoints are saved\n","        regardless of performance improvement.\n","    - starting_from (int, optional): The epoch number from which to start\n","        considering saving checkpoints based on loss improvement. Default: 20.\n","\n","    Returns:\n","    - None\n","    \"\"\"\n","\n","    # Check if this epoch's generator loss is an improvement\n","    if epoch_gener_loss < best_gen_loss and epoch > starting_from:\n","        # Save new best score\n","        best_gen_loss = epoch_gener_loss\n","\n","        # Log\n","        print(f\"New best generator loss {best_gen_loss} at epoch\",\n","              f\" {epoch}. Saving checkpoint.\")\n","\n","        # Save checkpoint\n","        hp.save_checkpoint(f\"best_{epoch}\",\n","                           epoch,\n","                           config,\n","                           PROJECT_DIR)\n","\n","    # Or maybe the checkpoint is saved due to the number of epochs\n","    elif epoch % save_step == 0:\n","        # Log\n","        print(f\"Epoch {epoch}. \",\n","              \"Not best losses, but saving checkpoint anyway.\")\n","\n","        # Save checkpoint\n","        hp.save_checkpoint(f\"epoch_{epoch}\",\n","                           epoch,\n","                           config,\n","                           PROJECT_DIR)"],"id":"JlPnBVMhyrpQ"},{"cell_type":"markdown","metadata":{"id":"IvZGCWsOYRlS"},"source":["## Training"],"id":"IvZGCWsOYRlS"},{"cell_type":"code","execution_count":null,"metadata":{"id":"KpCyntCQ91Ql"},"outputs":[],"source":["def train(config, verbose=True):\n","    \"\"\"\n","    Training loop\n","    \"\"\"\n","\n","    # Initialize a variable to track the best generator loss seen so far\n","    best_gen_loss = float('inf')\n","\n","    # Initialize lists of generator and discriminator losses per epoch\n","    gener_losses_epoch_list = []\n","    discr_losses_epoch_list = []\n","\n","    # Initialize configuration values from the config variable\n","    n_epochs = config.get('n_epochs', 100)\n","    crit_cycles = config.get('crit_cycles', 1)\n","    z_dim = config.get('z_dim', 100)\n","    show_step = config.get('show_step', 25)\n","    save_step = config.get('save_step', 5)\n","    last_epoch = config.get('epoch', 0)\n","    save_starting = config.get('save_starting', 20)\n","    penalty_fcn = config.get('penalty_fcn', lambda *x: 0)\n","\n","    # Initialize configuration values from the config object\n","    # config_dict = vars(config)  # Convert instance attributes to a dict\n","    dataloader = config['dataloader']\n","    gener = config['generator'].to(DEVICE)\n","    discr = config['discriminator'].to(DEVICE)\n","    gener_opt = config['g_optimizer']\n","    discr_opt = config['d_optimizer']\n","    loss_fcn = config['loss_fcn']\n","\n","    # Loop through epochs\n","    for epoch in range(last_epoch + 1, n_epochs + last_epoch + 1):\n","        # Initialize accumulators for losses\n","        epoch_gener_loss = 0.0\n","        epoch_discr_loss = 0.0\n","\n","        # Number of batches, used for averaging losses later\n","        num_batches = 0\n","\n","        # Loop through images\n","        for real_imgs, _ in tqdm(dataloader):\n","            num_batches += 1\n","\n","            # Current batch size\n","            current_bs = len(real_imgs)\n","\n","            # Pass real images to the gpu if available\n","            real_imgs = real_imgs.to(DEVICE)\n","\n","            # Train the discriminator (or critic) on real and fake images for\n","            # the number of cycles proposed.\n","            discr_loss_for_cycles = 0\n","            for _ in range(crit_cycles):\n","                # Zero gradients\n","                discr_opt.zero_grad()\n","\n","                # Generate noise (initial latent code of the generator)\n","                noise = torch.randn(current_bs, z_dim, device=DEVICE)\n","\n","                # Generate fake image from noise\n","                fake_imgs = gener(noise)\n","\n","                # Prediction of Discriminator on fake images (tensor must be\n","                # detached to avoid backpropagation on the generator weights)\n","                discr_fake_pred = discr(fake_imgs.detach())\n","\n","                # Losses of the Discriminator on fake images\n","                discr_fake_loss = loss_fcn(discr_fake_pred, label_type='fake')\n","\n","                # Prediction of Discriminator on real images\n","                discr_real_pred = discr(real_imgs)\n","\n","                # Losses of the Discriminator on real images\n","                discr_real_loss = loss_fcn(discr_real_pred, label_type='real')\n","\n","                # Calculate gradient penalty (not used in classic GAN)\n","                penalty = penalty_fcn(real_imgs,\n","                                      fake_imgs.detach(),\n","                                      discr)\n","\n","                # Discriminator (or critic) losses for the current cycle.\n","                discr_loss = discr_fake_loss + discr_real_loss + penalty\n","\n","                # Calculate losses of all the cycles so far\n","                discr_loss_for_cycles += discr_loss.item() / crit_cycles\n","\n","                # Backpropagation and weights update\n","                discr_loss.backward()\n","                discr_opt.step()\n","\n","            # Get final losses of the discriminator in the current epoch\n","            epoch_discr_loss += discr_loss_for_cycles\n","\n","            # Train the generator\n","\n","            # Zero gradients\n","            gener_opt.zero_grad()\n","\n","            # Generate noise (initial latent code of the generator)\n","            noise = torch.randn(current_bs, z_dim, device=DEVICE)\n","\n","            # Generate fake images from noise\n","            fake_imgs = gener(noise)\n","\n","            # Prediction of discriminator on fake images\n","            discr_fake_pred = discr(fake_imgs)\n","\n","            # Losses of the generator in the current batch\n","            gener_loss = loss_fcn(discr_fake_pred, label_type='real')\n","            epoch_gener_loss += gener_loss.item()\n","\n","            # Backpropagation and weights update\n","            gener_loss.backward()\n","            gener_opt.step()\n","\n","        # Average the losses for the current epoch\n","        epoch_gener_loss /= num_batches\n","        epoch_discr_loss /= num_batches\n","        gener_losses_epoch_list.append(epoch_gener_loss)\n","        discr_losses_epoch_list.append(epoch_discr_loss)\n","\n","        # Log of the epoch\n","        if verbose:\n","            print({'Epoch': epoch,\n","                   'Critic loss': epoch_discr_loss,\n","                   'Gen loss': epoch_gener_loss})\n","\n","        # Do checkpoint if required\n","        checkpointer(epoch=epoch,\n","                     epoch_gener_loss=epoch_gener_loss,\n","                     best_gen_loss=best_gen_loss,\n","                     config=config,\n","                     save_step=save_step,\n","                     starting_from=save_starting)\n","\n","        # Conditional visualization at the end of an epoch\n","        if epoch % show_step == 0:\n","            hp.visual_epoch(fake_imgs,\n","                            real_imgs,\n","                            gener_losses_epoch_list,\n","                            discr_losses_epoch_list)\n","\n","    return gener, discr, [gener_losses_epoch_list, discr_losses_epoch_list]"],"id":"KpCyntCQ91Ql"},{"cell_type":"markdown","source":["Define the training parameters."],"metadata":{"id":"bj65ryKApzJd"},"id":"bj65ryKApzJd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlj452ISwG8b"},"outputs":[],"source":["# Number of epochs to train.\n","n_epochs = 40\n","\n","# Define config dict\n","config = {}"],"id":"vlj452ISwG8b"},{"cell_type":"markdown","metadata":{"id":"yJx2exA6QPeL"},"source":["Let's do the training."],"id":"yJx2exA6QPeL"},{"cell_type":"code","source":["# Train\n","_ = train(config)"],"metadata":{"id":"I_fTVg0ZpaFV"},"id":"I_fTVg0ZpaFV","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oCX7n9vFNZ47"},"source":["# Generate images"],"id":"oCX7n9vFNZ47"},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbOJLk8twHFb"},"outputs":[],"source":["# Generate 25 latent codes of size 200\n","noise = torch.randn(25, z_dim, device=DEVICE)"],"id":"NbOJLk8twHFb"},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3BantCXm_zn"},"outputs":[],"source":["# Load the checkpoint indicated at 'name'\n","name = 'epoch_20'\n","hp.load_checkpoint(name, config, PROJECT_DIR)\n","\n","# Generate fake images\n","fake_imgs = config['generator'].eval().forward(noise)\n","\n","# Show fake images\n","hp.show(fake_imgs)"],"id":"h3BantCXm_zn"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6LssBIJlZJi6"},"outputs":[],"source":["# Load the checkpoint indicated at 'name'\n","name = 'epoch_40'\n","hp.load_checkpoint(name, config, PROJECT_DIR)\n","\n","# Generate fake images\n","fake_imgs = config['generator'].eval().forward(noise)\n","\n","# Show fake images\n","hp.show(fake_imgs)"],"id":"6LssBIJlZJi6"}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}