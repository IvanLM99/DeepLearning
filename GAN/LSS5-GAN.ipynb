{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1x-QAgitB-S5rxGGDqxsJ299ZQTfYtOhb\" width=180, align=\"center\"/>\n",
        "\n",
        "Master's degree in Intelligent Systems\n",
        "\n",
        "Subject: 11754 - Deep Learning\n",
        "\n",
        "Year: 2023-2024\n",
        "\n",
        "Professor: Miguel Ángel Calafat Torrens"
      ],
      "metadata": {
        "id": "PUu2VSg0UO91"
      },
      "id": "PUu2VSg0UO91"
    },
    {
      "cell_type": "markdown",
      "id": "auburn-therapy",
      "metadata": {
        "id": "auburn-therapy"
      },
      "source": [
        "# Lab 5 - GANs (Generative Adversarial Networks)\n",
        "\n",
        "Generative Adversarial Networks (GANs) was a groundbreaking approach in deep learning that revolutionized the field of artificial intelligence. As you have already seen in the theory of the subject, GANs consist of two neural networks –a generator and a discriminator– engaged in a competitive game. The generator aims to create realistic data samples, while the discriminator distinguishes between real and fake samples. Through adversarial training, GANs generate increasingly realistic data.\n",
        "\n",
        "This notebook covers GAN architecture and training, offering hands-on exercises to build and train models. You'll get to generate new data and grasp the core concepts of GANs. Let's get started."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classical GAN"
      ],
      "metadata": {
        "id": "4mUGsd-ysGs1"
      },
      "id": "4mUGsd-ysGs1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHdYAzcHm_y-"
      },
      "source": [
        "## Setting up\n",
        "\n",
        "In this notebook we will use the celebA dataset. You can find it in a lot of places all over the internet, but one good place to start is its own [website](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) where you scroll down to the \"**Download**\" section, select the link \"**In the wild images**\" go into the folder \"**Img**\" and download the file \"**image_align_celeba.zip**\". Maybe it's even easier download it from this [link of Kaggle](https://www.kaggle.com/jessicali9530/celeba-dataset) if you have an account (the account is free).\n",
        "\n",
        "Anyway, you just remember that **the CelebA dataset is available for non-commercial research purposes only**."
      ],
      "id": "QHdYAzcHm_y-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is 1.34GB in size because it has more than 200,000 images. In this practice, since what is done is a proof of concept, you do not need to upload all the images. A good option would be to take a selection of, for example, 2000 photos and compress them into a file that will be the one you upload to your Drive account.\n",
        "\n",
        "In my case I've taken the first 2000 images of celebA and compressed them into a file called `img_align_celeba_small.zip` that I've uploaded to my GDrive."
      ],
      "metadata": {
        "id": "B7lhDvFhbOgY"
      },
      "id": "B7lhDvFhbOgY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to your drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')\n",
        "%cd '/content/gdrive/MyDrive/LABS2024/LAB5'\n",
        "%ls -l\n",
        "\n",
        "# Here the path of the project folder (which is where this file is) is inserted\n",
        "# into the python path.\n",
        "import pathlib\n",
        "import sys\n",
        "import os\n",
        "import helper_PR5 as hp\n",
        "\n",
        "PROJECT_DIR = str(pathlib.Path().resolve())\n",
        "sys.path.append(PROJECT_DIR)"
      ],
      "metadata": {
        "id": "gP0_MIRNnQqY"
      },
      "id": "gP0_MIRNnQqY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure you write down the correct path to your zip file with the dataset\n",
        "\n",
        "# CelebA full\n",
        "# dataset_zip_fullpath = '/content/gdrive/MyDrive/datasets/img_align_celeba.zip'\n",
        "\n",
        "# CelebA small\n",
        "dataset_zip_fullpath = '/content/gdrive/MyDrive/datasets/img_align_celeba_small.zip'"
      ],
      "metadata": {
        "id": "NH1OBO-Gz-CH"
      },
      "id": "NH1OBO-Gz-CH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before running the next cell, make sure you have left the zip file in the location you want and assign the variable accordingly.**"
      ],
      "metadata": {
        "id": "H1mizjwJg0i9"
      },
      "id": "H1mizjwJg0i9"
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell will take several minutes the first time you run it.\n",
        "\n",
        "# You can execute this cell every time you run the code. It won't unzip\n",
        "# anything if the dataset is already unzipped.\n",
        "DATA_DIR = hp.extract_dataset(dataset_zip_fullpath,\n",
        "                              remove_zip=True)"
      ],
      "metadata": {
        "id": "iOSH1KeJz-Jq"
      },
      "id": "iOSH1KeJz-Jq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps, the first time you run this notebook, you might prefer using the CPU environment, because you'll need some time to understand what is being done (remember the GPU usage limitations set by Colab). Moreover, the GPU will only be necessary when you wish to execute the trainings, that is, when you know how the entire set works."
      ],
      "metadata": {
        "id": "TLGNkY3Sl-ic"
      },
      "id": "TLGNkY3Sl-ic"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import PIL\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "dDFNPJb34Qkg"
      },
      "id": "dDFNPJb34Qkg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "# If you have made changes to the hp module and want to reload it\n",
        "importlib.reload(hp)"
      ],
      "metadata": {
        "id": "8zEzatxr-odI"
      },
      "id": "8zEzatxr-odI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The dataset\n",
        "\n",
        "The dataset is made up of images of faces of famous people. Taking into account that we do not want to do very expensive computational training, we are not interested in using the images with their original resolution (218 x 178), but it is better for us to convert them to a lower resolution, which in this case will be 64 x 64.\n",
        "\n",
        "Note that a custom dataset class is defined in the next cell, in a similar way that you did in previous practices. It uses the `__getitem__()` method to implement the corresponding transformations. In addition to the image, a label is returned, which in this case is not necessary. The label is the name of the file."
      ],
      "metadata": {
        "id": "IDAWZsI9ryjX"
      },
      "id": "IDAWZsI9ryjX"
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_folder, lim=2000, transforms=None):\n",
        "        # lim is the max number of images that we want to use. It's default is\n",
        "        # used when doing a prove of concept, so that we don't want to use all\n",
        "        # images of the dataset\n",
        "        self.img_folder = img_folder\n",
        "        self.lim = lim\n",
        "\n",
        "        # Initialize empty lists for items and labels\n",
        "        self.items = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Walk through all files in img_folder and its subfolders\n",
        "        for root, _, files in os.walk(img_folder):\n",
        "            for file in files:\n",
        "                # Check if the file is an image\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    full_path = os.path.join(root, file)\n",
        "                    self.items.append(full_path)\n",
        "                    self.labels.append(file)\n",
        "\n",
        "                    # Stop if the limit is reached\n",
        "                    if lim > 0 and len(self.items) >= lim:\n",
        "                        break\n",
        "\n",
        "            if lim > 0 and len(self.items) >= lim:\n",
        "                break\n",
        "\n",
        "        # Define the transformation pipeline\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load the image\n",
        "        data = PIL.Image.open(self.items[idx]).convert('RGB')\n",
        "\n",
        "        # Apply the transformations if provided\n",
        "        if self.transforms:\n",
        "            data = self.transforms(data)\n",
        "\n",
        "        # Return the processed data and its corresponding label\n",
        "        return data, self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)"
      ],
      "metadata": {
        "id": "JGGx2Q0UiMki"
      },
      "id": "JGGx2Q0UiMki",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well, we now have the dataset object, but before instantiating it we need to know what transformations we are going to apply.\n",
        "Later you will see how the generator typically uses an activation function `tanh`, whose output gives a value between -1 and 1. For this reason it is recommended that the input tensors are already scaled in this range."
      ],
      "metadata": {
        "id": "zPtbytIog9Ha"
      },
      "id": "zPtbytIog9Ha"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformations to be done\n",
        "\n",
        "# As you know, transforms.ToTensor transforms the image into a tensor and\n",
        "# scales it in the range [0, 1]. In this case we use this class to define a new\n",
        "# class that fulfills the purpose of scaling in the range [-1, 1] using the\n",
        "# first one as the base class.\n",
        "\n",
        "# Be sure you understand this code and why it works\n",
        "\n",
        "class ToScaledTensor(transforms.ToTensor):\n",
        "    def __init__(self, low=-1, high=1):\n",
        "        super().__init__()\n",
        "        self.low = low\n",
        "        self.high = high\n",
        "\n",
        "    def __call__(self, img):\n",
        "        # Convert image to tensor (same as ToTensor)\n",
        "        tensor = super().__call__(img)\n",
        "        # Scale the tensor to the desired range\n",
        "        tensor = tensor * (self.high - self.low) + self.low\n",
        "        return tensor"
      ],
      "metadata": {
        "id": "k8ITu66ALYQu"
      },
      "id": "k8ITu66ALYQu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can define the transformations in the usual way. The first step is to crop the original image (218 x 178) to create a centered square image (178 x 178), which is the largest size possible.. This image does not crop faces in the vast majority of cases.\n",
        "Then we will resize the image to the required dimension (64 x 64)."
      ],
      "metadata": {
        "id": "xA0Ow6vokel1"
      },
      "id": "xA0Ow6vokel1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of the images (height and width)\n",
        "img_size = 64\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    # Center crop the images so that they become square\n",
        "    transforms.CenterCrop((178, 178)),\n",
        "    # Resize the image to the specified size (h, w)\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    # Convert the image to a PyTorch tensor and scale to [-1, 1]\n",
        "    ToScaledTensor(),\n",
        "    ])"
      ],
      "metadata": {
        "id": "9WP9IA3qkfLS"
      },
      "id": "9WP9IA3qkfLS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, finally, we are ready to instantiate the dataset."
      ],
      "metadata": {
        "id": "KMit25N5lTiV"
      },
      "id": "KMit25N5lTiV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Max number of images to use (-1 if all)\n",
        "\n",
        "# Maybe you want to start working with fewer images to see how it works\n",
        "limit = 2000\n",
        "\n",
        "# Define the custom dataset object.\n",
        "dataset = CustomDataset(DATA_DIR,\n",
        "                        transforms=transform,\n",
        "                        lim=limit)"
      ],
      "metadata": {
        "id": "tmeobSLCqrby"
      },
      "id": "tmeobSLCqrby",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The dataloader"
      ],
      "metadata": {
        "id": "wgkCh2Usfx40"
      },
      "id": "wgkCh2Usfx40"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to instantiate the dataloader."
      ],
      "metadata": {
        "id": "qQ8vto3MosDL"
      },
      "id": "qQ8vto3MosDL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size (number of images per batch)\n",
        "batch_size = 128\n",
        "\n",
        "# Define the DataLoader\n",
        "dataloader = DataLoader(dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=True)"
      ],
      "metadata": {
        "id": "XdlikL51Ctoy"
      },
      "id": "XdlikL51Ctoy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the following cell a helper function is used. I recommend that you check these helper functions to fully understand how they work."
      ],
      "metadata": {
        "id": "erL89BdBuC-1"
      },
      "id": "erL89BdBuC-1"
    },
    {
      "cell_type": "code",
      "source": [
        "# And now let's see images of the first batch\n",
        "hp.show(next(iter(dataloader))[0])"
      ],
      "metadata": {
        "id": "QQbnw98PAZQ4"
      },
      "id": "QQbnw98PAZQ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The networks\n",
        "\n",
        "Now the time has come to define the two competing models. On the one hand we have the generator, which will have a decoder configuration, and on the other hand we will have the discriminator, which will have an encoder configuration.\n",
        "\n",
        "They can be considered symmetrical networks because one attempts to generate a credible image from a latent space, while the other analyzes an image to determine its authenticity by mapping it back to a latent space.\n",
        "\n",
        "Below you'll find a representation of the tensor's shape that we will have at each step. Note, for example, that in the first downsample step (CONV in the image) of the discriminator it recieves as input an image tensor of shape (3, 64, 64) (3 channels, 64 pixels height, 64 pixels width) and outputs a tensor of (64, 32, 32) due to the number os kernels applied, and the factor 2 reduction with the stride.\n",
        "\n",
        "In the following downsample steps the width is doubled and the height and width are divided by 2. At the end of the convolutional layers the tensor of shape (512, 4, 4) is squeezed in a tensor of 8192 values, and finally, through a fully connected layer, it outputs a single value between 0 and 1 that indicates wether the image is fake or real."
      ],
      "metadata": {
        "id": "mj_oCYEGIscL"
      },
      "id": "mj_oCYEGIscL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1WXTHp5vY2rEzVZRAoGsfoAZ4pq6kCb6d\" width=700, align=\"center\"/>"
      ],
      "metadata": {
        "id": "MjL7dz4QNFNH"
      },
      "id": "MjL7dz4QNFNH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define the discriminator with an encoder estructure of 4 downscale\n",
        "# steps.\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_dim=64, img_size=64):\n",
        "\n",
        "        # \"d_dim\" is the output dimension of the first convolutional layer;\n",
        "        # that is, the number of filters/kernels you have in the first layer\n",
        "        # (3 inputs, channels RGB, and d_dim outputs)\n",
        "        super().__init__()\n",
        "\n",
        "        # Configuration parameters\n",
        "        kernel_size = 4\n",
        "        n = 4  # Number of conv layers. Only used for definition of fc_in\n",
        "        fc_in = int(d_dim * 2**(n-1) * (img_size/(2**n))**2)  # fc input dim\n",
        "        pad = 1\n",
        "        stride = 2\n",
        "        bias = False\n",
        "\n",
        "        # Helper function for convolutions\n",
        "        def conv(in_channels, out_channels):\n",
        "            return nn.Conv2d(in_channels=in_channels,\n",
        "                             out_channels=out_channels,\n",
        "                             kernel_size=kernel_size,\n",
        "                             stride=stride,\n",
        "                             padding=pad,\n",
        "                             bias=bias)\n",
        "\n",
        "        # Dense or fully connected layer\n",
        "        self.fc = nn.Linear(fc_in, 1)  # d_dim x 2^3 x (img_size / 2^4)^2 = 8192\n",
        "\n",
        "        # Convolutional layers (doubling the depth with each step)\n",
        "        self.conv1 = conv(in_channels=3,\n",
        "                          out_channels=d_dim)\n",
        "        self.conv2 = conv(in_channels=d_dim,\n",
        "                          out_channels=2*d_dim)\n",
        "        self.conv3 = conv(in_channels=2*d_dim,\n",
        "                          out_channels=4*d_dim)\n",
        "        self.conv4 = conv(in_channels=4*d_dim,\n",
        "                          out_channels=8*d_dim)\n",
        "\n",
        "        # Batchnorm layers. Not usually applied to the first convolutional layer\n",
        "        self.bnorm2 = nn.BatchNorm2d(2 * d_dim)\n",
        "        self.bnorm3 = nn.BatchNorm2d(4 * d_dim)\n",
        "        self.bnorm4 = nn.BatchNorm2d(8 * d_dim)\n",
        "\n",
        "        # Define a LeakyReLU activation function layer\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: batch x 3 x img_size x img_size  -->  b x 3 x 64 x 64\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # See output shape at each step\n",
        "        out = self.leaky_relu(self.conv1(x))                 # b x 64 x 32 x 32\n",
        "        out = self.leaky_relu(self.bnorm2(self.conv2(out)))  # b x 128 x 16 x 16\n",
        "        out = self.leaky_relu(self.bnorm3(self.conv3(out)))  # b x 256 x 8 x 8\n",
        "        out = self.leaky_relu(self.bnorm4(self.conv4(out)))  # b x 512 x 4 x 4\n",
        "\n",
        "        # Flatten (b x 512 x 4 x 4  ==  b x 8192)\n",
        "        out = out.contiguous().view(batch_size, -1)  # b x 8192\n",
        "\n",
        "        # Final output layer without activation function\n",
        "        scores = self.fc(out)  # b x 1\n",
        "        return scores"
      ],
      "metadata": {
        "id": "kq3t7N34z-VY"
      },
      "id": "kq3t7N34z-VY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generator, as you can see in the image below, has the reversed configuration. In this case we start with a latent code of 100 values, which is converted via a fully connected layer into a tensor of 8192 positions. Afterwards, it is viewed as (512, 4, 4) tensor and them back to the shape of a tensor image using 4 steps of transposed convolutional layers."
      ],
      "metadata": {
        "id": "fFi4b2OODynq"
      },
      "id": "fFi4b2OODynq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The typical generator configuration is that of a decoder; that is, it is the opposite of the discriminator.\n"
      ],
      "metadata": {
        "id": "MVnBGU2r-IbF"
      },
      "id": "MVnBGU2r-IbF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1BMvNbC-OK0oINFG0Oz9nmfoN9lwppVw1\" width=700, align=\"center\"/>"
      ],
      "metadata": {
        "id": "eOV5c2PcOaYx"
      },
      "id": "eOV5c2PcOaYx"
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    # 'd_dim' is the input dimension of the last transposed convolutional layer;\n",
        "    # that is, the output of this layer has to be of depth 3 due to RGB, but the\n",
        "    # input is arbitrary. Well, here it will be 'd_dim', that is the counterpart\n",
        "    # of the 'd_dim' defined in the discriminator.\n",
        "\n",
        "    # 'z_dim' is the size of the input to the network.\n",
        "\n",
        "    # You design the forward pass from the end to the beginning by calculating\n",
        "    # the dimensions (see comments in the code)\n",
        "\n",
        "    def __init__(self, z_dim=100, d_dim=64, img_size=64):\n",
        "        super().__init__()\n",
        "\n",
        "        # Configuration parameters\n",
        "        kernel_size = 4\n",
        "        self.d_dim = d_dim\n",
        "\n",
        "        # Number of convolutional layers (only for sizes calculation purposes,\n",
        "        # i.e. not used dinamically to define a different model)\n",
        "        n = 4\n",
        "\n",
        "        # Calculate dimensions of fully connected layer\n",
        "        fc_out = int(d_dim * 2**(n-1) * (img_size/(2**n))**2)\n",
        "        # Image size (height and width) on fully connected layer output\n",
        "        self.i_s_fco = int(img_size / 2**n)\n",
        "\n",
        "        # Configuration data for transposed convolutional layers\n",
        "        pad = 1\n",
        "        stride = 2\n",
        "        bias = False\n",
        "        kernel_size = 4\n",
        "\n",
        "        # Helper function for transposed convolutions\n",
        "        def tconv(in_channels, out_channels):\n",
        "            return nn.ConvTranspose2d(in_channels=in_channels,\n",
        "                                      out_channels=out_channels,\n",
        "                                      kernel_size=kernel_size,\n",
        "                                      stride=stride,\n",
        "                                      padding=pad,\n",
        "                                      bias=bias)\n",
        "\n",
        "        # Linear layer\n",
        "        self.fc = nn.Linear(z_dim, fc_out)\n",
        "\n",
        "        # Transpose convolutional layers\n",
        "        self.tconv1 = tconv(in_channels=d_dim*8,\n",
        "                            out_channels=d_dim*4)\n",
        "        self.tconv2 = tconv(in_channels=d_dim*4,\n",
        "                            out_channels=d_dim*2)\n",
        "        self.tconv3 = tconv(in_channels=d_dim*2,\n",
        "                            out_channels=d_dim)\n",
        "        self.tconv4 = tconv(in_channels=d_dim,\n",
        "                            out_channels=3)\n",
        "\n",
        "        # Batchnorm layers\n",
        "        self.bnorm1 = nn.BatchNorm2d(d_dim * 4)\n",
        "        self.bnorm2 = nn.BatchNorm2d(d_dim * 2)\n",
        "        self.bnorm3 = nn.BatchNorm2d(d_dim)\n",
        "\n",
        "        # Define activation function layers\n",
        "        self.relu = nn.ReLU(True)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Input size: b x z_dim = b x 100\n",
        "        b = x.size(0)\n",
        "\n",
        "        # First of all transform the input tensor (the latent generator's code)\n",
        "        # to dimensions equivalent to those of the latent space of the\n",
        "        # discriminator, i.e: b x d_dim*8 x img_size/16 x img_size/16\n",
        "        out = self.fc(x)                                # b x 8192\n",
        "        out = out.contiguous().view(b, -1,\n",
        "                                    self.i_s_fco,\n",
        "                                    self.i_s_fco)       # b x 512 x 4 x 4\n",
        "\n",
        "        # Now apply the transposed convolutions\n",
        "        out = self.relu(self.bnorm1(self.tconv1(out)))  # b x 256 x 8 x 8\n",
        "        out = self.relu(self.bnorm2(self.tconv2(out)))  # b x 128 x 16 x 16\n",
        "        out = self.relu(self.bnorm3(self.tconv3(out)))  # b x 64 x 32 x 32\n",
        "        out = self.tanh(self.tconv4(out))               # b x 3 x 64 x 64\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Wf-Wv0Lt91Br"
      },
      "id": "Wf-Wv0Lt91Br",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An now you can instantiate the models. You can use the default parameters or, if you wish, you can redefine them if you want to conduct some tests."
      ],
      "metadata": {
        "id": "4qibf6jCdx8d"
      },
      "id": "4qibf6jCdx8d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the models with default parameters\n",
        "D = Discriminator()\n",
        "G = Generator()"
      ],
      "metadata": {
        "id": "UPeo51DI91H3"
      },
      "id": "UPeo51DI91H3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizers\n",
        "\n",
        "In this section, we configure the optimizers with the same values for the learning rate and beta parameters. While you have the option to adjust these values, it's not necessary, as the provided parameters are already well-suited for most purposes."
      ],
      "metadata": {
        "id": "KvXzuZuJPJb0"
      },
      "id": "KvXzuZuJPJb0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator optimizer\n",
        "\n",
        "# Parameters\n",
        "lr_d = 0.0004\n",
        "beta1 = 0.6\n",
        "beta2 = 0.999\n",
        "\n",
        "# Create optimizer for the discriminator D\n",
        "d_optimizer = torch.optim.Adam(D.parameters(), lr_d, [beta1, beta2])"
      ],
      "metadata": {
        "id": "JRLlIETn8ds1"
      },
      "id": "JRLlIETn8ds1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator optimizer\n",
        "\n",
        "# Parameters\n",
        "lr_g = 0.0004\n",
        "beta1 = 0.6\n",
        "beta2 = 0.999\n",
        "\n",
        "# Create optimizers for generator G\n",
        "g_optimizer = torch.optim.Adam(G.parameters(), lr_g, [beta1, beta2])"
      ],
      "metadata": {
        "id": "dLrf3Ywn8dzV"
      },
      "id": "dLrf3Ywn8dzV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Losses"
      ],
      "metadata": {
        "id": "zzTb7d5HYvtD"
      },
      "id": "zzTb7d5HYvtD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The issue of the loss function is especially controversial in the case of GANs. Until now, we have encountered loss functions that influence only a single model. However, in the scenario of GANs, a single loss function must govern two distinct models with fundamentally opposing goals.\n",
        "\n",
        "Initially, we plan to use Binary Cross Entropy Loss. However, because the discriminator's output lacks an activation function, we will opt for the variant that directly applies to logits or scores; namely, BCEWithLogitsLoss.\n",
        "\n",
        "Note that the loss function below takes the output of the discriminator as input, but it does not receive the ground truth (i.e., the correct labels identifying whether an image is real or fake). Instead, it accepts a keyword argument called \"label_type\", which has only two possible values: 'real' or 'fake'. Thus, when training the discriminator with real images, we will assign the value 'real' to this parameter, and when training with fake images, we will assign 'fake'.\n",
        "\n",
        "Please review the code below to ensure you understand it."
      ],
      "metadata": {
        "id": "xeYFUMKoPWoy"
      },
      "id": "xeYFUMKoPWoy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function\n",
        "def gan_loss_fcn(discr_output, **kwargs):\n",
        "    \"\"\"\n",
        "    Calculates the loss based on specified label type (real or fake).\n",
        "\n",
        "    Args:\n",
        "        discr_output: Tensor of discriminator logits.\n",
        "        **kwargs: Keyword arguments, including:\n",
        "            - label_type (str, optional): Label type, either \"real\" or \"fake\".\n",
        "              Defaults to \"real\".\n",
        "            - smooth (bool, optional): Apply label smoothing on real labels.\n",
        "              Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        A Tensor containing the calculated discriminator loss.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = discr_output.size(0)\n",
        "\n",
        "    # Extract label_type and smooth from **kwargs with defaults\n",
        "    label_type = kwargs.get(\"label_type\", \"real\")\n",
        "    smooth = kwargs.get(\"smooth\", False)\n",
        "\n",
        "    # Ensure label_type is valid (real or fake)\n",
        "    label_type = \"real\" if label_type.lower() not in (\"real\", \"fake\") \\\n",
        "        else label_type.lower()\n",
        "\n",
        "    # Define labels based on label_type\n",
        "    if label_type == \"real\":\n",
        "        labels = torch.ones(batch_size) * (0.9 if smooth else 1.0)\n",
        "    else:\n",
        "        labels = torch.zeros(batch_size)\n",
        "\n",
        "    # Move labels to GPU if available\n",
        "    labels = labels.to(DEVICE)\n",
        "\n",
        "    # Binary cross entropy with logits loss\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss = criterion(discr_output.squeeze(), labels)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "oELxtZBrxhaw"
      },
      "id": "oELxtZBrxhaw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpoints"
      ],
      "metadata": {
        "id": "1kgcTmtHyrRC"
      },
      "id": "1kgcTmtHyrRC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, we're going to perform checkpoints. The code below is a function that will be called within the training function. It will assess whether a checkpoint is required, and if so, it will proceed by utilizing a helper function from `helper_PR5.py`.\n",
        "\n",
        "Check the criteria used to perform or not the checkpoint."
      ],
      "metadata": {
        "id": "rW0fGckzSa40"
      },
      "id": "rW0fGckzSa40"
    },
    {
      "cell_type": "code",
      "source": [
        "def checkpointer(epoch,\n",
        "                 epoch_gener_loss,\n",
        "                 best_gen_loss,\n",
        "                 config,\n",
        "                 save_step,\n",
        "                 starting_from=20):\n",
        "    \"\"\"\n",
        "    Do checkpoints if required.\n",
        "\n",
        "    Evaluates the performance of the generator at the current epoch and decides\n",
        "    whether to save a checkpoint. Checkpoints are saved if the generator's loss\n",
        "    is improved past a specified epoch or at regular intervals defined by the\n",
        "    save_step parameter.\n",
        "\n",
        "    The function logs the reason for saving a checkpoint, updates the best\n",
        "    generator loss if necessary, and calls a separate function to actually save\n",
        "    the checkpoint.\n",
        "\n",
        "    Parameters:\n",
        "    - epoch (int): The current training epoch.\n",
        "    - epoch_gener_loss (float): The generator's loss for the current epoch.\n",
        "    - best_gen_loss (float): The best generator loss observed so far in the\n",
        "        training.\n",
        "    - config (dict): A dictionary containing the configuration of the model,\n",
        "        including the model itself and its optimizer.\n",
        "    - save_step (int): The interval of epochs at which checkpoints are saved\n",
        "        regardless of performance improvement.\n",
        "    - starting_from (int, optional): The epoch number from which to start\n",
        "        considering saving checkpoints based on loss improvement. Default: 20.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if this epoch's generator loss is an improvement\n",
        "    if epoch_gener_loss < best_gen_loss and epoch > starting_from:\n",
        "        # Save new best score\n",
        "        best_gen_loss = epoch_gener_loss\n",
        "\n",
        "        # Log\n",
        "        print(f\"New best generator loss {best_gen_loss} at epoch\",\n",
        "              f\" {epoch}. Saving checkpoint.\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        hp.save_checkpoint(f\"best_{epoch}\",\n",
        "                           epoch,\n",
        "                           config,\n",
        "                           PROJECT_DIR)\n",
        "\n",
        "    # Or maybe the checkpoint is saved due to the number of epochs\n",
        "    elif epoch % save_step == 0:\n",
        "        # Log\n",
        "        print(f\"Epoch {epoch}. \",\n",
        "              \"Not best losses, but saving checkpoint anyway.\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        hp.save_checkpoint(f\"epoch_{epoch}\",\n",
        "                           epoch,\n",
        "                           config,\n",
        "                           PROJECT_DIR)"
      ],
      "metadata": {
        "id": "JlPnBVMhyrpQ"
      },
      "id": "JlPnBVMhyrpQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "IvZGCWsOYRlS"
      },
      "id": "IvZGCWsOYRlS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we reach the training phase, which, as you know, involves training the two models in a competition against each other.\n",
        "\n",
        "Given that the training function is quite extensive, it's recommended to be analyzed in detail. Notice that within the loop through batches, both the discriminator and the generator are trained. Moreover, within this same sweep, a new loop is introduced that could train the discriminator more times than the generator."
      ],
      "metadata": {
        "id": "DBrI9fHXxJJS"
      },
      "id": "DBrI9fHXxJJS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1oZ6TVh1dkRIHIBmuovS2QM1yYgVSLgz3\" width=900, align=\"center\"/>"
      ],
      "metadata": {
        "id": "N73RIxd9NeAe"
      },
      "id": "N73RIxd9NeAe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In short, an overview of what you'll find in the training function can be seen in the following scheme in pseudocode."
      ],
      "metadata": {
        "id": "b3HuZE7rN52Y"
      },
      "id": "b3HuZE7rN52Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1gtHLkmCqJRmpPwfY9PVJA3BZ33-k0qAz\" width=500, align=\"center\"/>"
      ],
      "metadata": {
        "id": "zVYWk7GvEv6r"
      },
      "id": "zVYWk7GvEv6r"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config, verbose=True):\n",
        "    \"\"\"\n",
        "    Training loop\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize a variable to track the best generator loss seen so far\n",
        "    best_gen_loss = float('inf')\n",
        "\n",
        "    # Initialize lists of generator and discriminator losses per epoch\n",
        "    gener_losses_epoch_list = []\n",
        "    discr_losses_epoch_list = []\n",
        "\n",
        "    # Initialize configuration values from the config variable\n",
        "    n_epochs = config.get('n_epochs', 100)\n",
        "    crit_cycles = config.get('crit_cycles', 1)\n",
        "    z_dim = config.get('z_dim', 100)\n",
        "    show_step = config.get('show_step', 25)\n",
        "    save_step = config.get('save_step', 5)\n",
        "    last_epoch = config.get('epoch', 0)\n",
        "    save_starting = config.get('save_starting', 20)\n",
        "    penalty_fcn = config.get('penalty_fcn', lambda *x: 0)\n",
        "\n",
        "    # Initialize configuration values from the config object\n",
        "    # config_dict = vars(config)  # Convert instance attributes to a dict\n",
        "    dataloader = config['dataloader']\n",
        "    gener = config['generator'].to(DEVICE)\n",
        "    discr = config['discriminator'].to(DEVICE)\n",
        "    gener_opt = config['g_optimizer']\n",
        "    discr_opt = config['d_optimizer']\n",
        "    loss_fcn = config['loss_fcn']\n",
        "\n",
        "    # Loop through epochs\n",
        "    for epoch in range(last_epoch + 1, n_epochs + last_epoch + 1):\n",
        "        # Initialize accumulators for losses\n",
        "        epoch_gener_loss = 0.0\n",
        "        epoch_discr_loss = 0.0\n",
        "\n",
        "        # Number of batches, used for averaging losses later\n",
        "        num_batches = 0\n",
        "\n",
        "        # Loop through images\n",
        "        for real_imgs, _ in tqdm(dataloader):\n",
        "            num_batches += 1\n",
        "\n",
        "            # Current batch size\n",
        "            current_bs = len(real_imgs)\n",
        "\n",
        "            # Pass real images to the gpu if available\n",
        "            real_imgs = real_imgs.to(DEVICE)\n",
        "\n",
        "            # Train the discriminator (or critic) on real and fake images for\n",
        "            # the number of cycles proposed.\n",
        "            discr_loss_for_cycles = 0\n",
        "            for _ in range(crit_cycles):\n",
        "                # Zero gradients\n",
        "                discr_opt.zero_grad()\n",
        "\n",
        "                # Generate noise (initial latent code of the generator)\n",
        "                noise = torch.randn(current_bs, z_dim, device=DEVICE)\n",
        "\n",
        "                # Generate fake image from noise\n",
        "                fake_imgs = gener(noise)\n",
        "\n",
        "                # Prediction of Discriminator on fake images (tensor must be\n",
        "                # detached to avoid backpropagation on the generator weights)\n",
        "                discr_fake_pred = discr(fake_imgs.detach())\n",
        "\n",
        "                # Losses of the Discriminator on fake images\n",
        "                discr_fake_loss = loss_fcn(discr_fake_pred, label_type='fake')\n",
        "\n",
        "                # Prediction of Discriminator on real images\n",
        "                discr_real_pred = discr(real_imgs)\n",
        "\n",
        "                # Losses of the Discriminator on real images\n",
        "                discr_real_loss = loss_fcn(discr_real_pred, label_type='real')\n",
        "\n",
        "                # Calculate gradient penalty (not used in classic GAN)\n",
        "                penalty = penalty_fcn(real_imgs,\n",
        "                                      fake_imgs.detach(),\n",
        "                                      discr)\n",
        "\n",
        "                # Discriminator (or critic) losses for the current cycle.\n",
        "                discr_loss = discr_fake_loss + discr_real_loss + penalty\n",
        "\n",
        "                # Calculate losses of all the cycles so far\n",
        "                discr_loss_for_cycles += discr_loss.item() / crit_cycles\n",
        "\n",
        "                # Backpropagation and weights update\n",
        "                discr_loss.backward()\n",
        "                discr_opt.step()\n",
        "\n",
        "            # Get final losses of the discriminator in the current epoch\n",
        "            epoch_discr_loss += discr_loss_for_cycles\n",
        "\n",
        "            # Train the generator\n",
        "\n",
        "            # Zero gradients\n",
        "            gener_opt.zero_grad()\n",
        "\n",
        "            # Generate noise (initial latent code of the generator)\n",
        "            noise = torch.randn(current_bs, z_dim, device=DEVICE)\n",
        "\n",
        "            # Generate fake images from noise\n",
        "            fake_imgs = gener(noise)\n",
        "\n",
        "            # Prediction of discriminator on fake images\n",
        "            discr_fake_pred = discr(fake_imgs)\n",
        "\n",
        "            # Losses of the generator in the current batch\n",
        "            gener_loss = loss_fcn(discr_fake_pred, label_type='real')\n",
        "            epoch_gener_loss += gener_loss.item()\n",
        "\n",
        "            # Backpropagation and weights update\n",
        "            gener_loss.backward()\n",
        "            gener_opt.step()\n",
        "\n",
        "        # Average the losses for the current epoch\n",
        "        epoch_gener_loss /= num_batches\n",
        "        epoch_discr_loss /= num_batches\n",
        "        gener_losses_epoch_list.append(epoch_gener_loss)\n",
        "        discr_losses_epoch_list.append(epoch_discr_loss)\n",
        "\n",
        "        # Log of the epoch\n",
        "        if verbose:\n",
        "            print({'Epoch': epoch,\n",
        "                   'Critic loss': epoch_discr_loss,\n",
        "                   'Gen loss': epoch_gener_loss})\n",
        "\n",
        "        # Do checkpoint if required\n",
        "        checkpointer(epoch=epoch,\n",
        "                     epoch_gener_loss=epoch_gener_loss,\n",
        "                     best_gen_loss=best_gen_loss,\n",
        "                     config=config,\n",
        "                     save_step=save_step,\n",
        "                     starting_from=save_starting)\n",
        "\n",
        "        # Conditional visualization at the end of an epoch\n",
        "        if epoch % show_step == 0:\n",
        "            hp.visual_epoch(fake_imgs,\n",
        "                            real_imgs,\n",
        "                            gener_losses_epoch_list,\n",
        "                            discr_losses_epoch_list)\n",
        "\n",
        "    return gener, discr, [gener_losses_epoch_list, discr_losses_epoch_list]"
      ],
      "metadata": {
        "id": "KpCyntCQ91Ql"
      },
      "id": "KpCyntCQ91Ql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's do the first training."
      ],
      "metadata": {
        "id": "yJx2exA6QPeL"
      },
      "id": "yJx2exA6QPeL"
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to run some trial trainings you can tweak the number of images\n",
        "# of the dataset (limit) or the epochs to reduce the time invested.\n",
        "\n",
        "# Note that with this configuration we don't save checkpoints\n",
        "\n",
        "# Number of epochs to train.\n",
        "n_epochs = 40\n",
        "\n",
        "# Let's embed all the necessary data in single dict variable\n",
        "config = {'n_epochs': n_epochs,         # Number of epochs to train\n",
        "          'dataloader': dataloader,\n",
        "          'discriminator': D,\n",
        "          'generator': G,\n",
        "          'd_optimizer': d_optimizer,\n",
        "          'g_optimizer': g_optimizer,\n",
        "          'project_dir': PROJECT_DIR,   # The folder where checkpoints will be saved\n",
        "          'loss_fcn': gan_loss_fcn,     # The loss function\n",
        "          'show_step': 5,               # Show images every 'show_step' steps\n",
        "          'save_step': 1000,            # Save checkpoint every 'save_step' epochs\n",
        "          'save_starting': 1000,        # First epoch to save checkpoints when improving\n",
        "          }"
      ],
      "metadata": {
        "id": "6zuGQxko8d7p"
      },
      "id": "6zuGQxko8d7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = train(config)"
      ],
      "metadata": {
        "id": "cHeF3wVwY3kr"
      },
      "id": "cHeF3wVwY3kr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WGAN-GP\n",
        "\n",
        "Training GANs often encounters problems like mode collapse, where the generator produces limited varieties of samples, and non-convergence, where the generator and discriminator keep outperforming each other without stabilization.\n",
        "\n",
        "In this section we are going to implement one option to help dealing with non-convergence problems.\n",
        "\n",
        "The Wasserstein GAN (WGAN) addresses non-convergence by using a different loss function known as the Wasserstein loss, or Earth Mover's distance. This approach improves training stability and provides more meaningful gradients, making it easier for the GAN to learn. It involves constraining the discriminator (now called a critic) to be a 1-Lipschitz function, which is typically enforced through weights clipping or, more recently, through gradient penalty (WGAN-GP), helping the model to converge more reliably.\n",
        "\n",
        "[Wassestein GAN with gradient penalty](https://paperswithcode.com/method/wgan-gp)\n",
        "\n",
        "Moreover, by implementing gradient penalty, it enforces a smoother gradient behavior for the critic. This smoother gradient behavior provides better training signals to the generator, encouraging it to produce more diverse samples and thereby also reducing the risk of mode collapse.\n"
      ],
      "metadata": {
        "id": "bgGkUoIRmlVd"
      },
      "id": "bgGkUoIRmlVd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The critic"
      ],
      "metadata": {
        "id": "rNkqa-DQ9VwA"
      },
      "id": "rNkqa-DQ9VwA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the model below is dimensionally identical to the Discriminator of the classical GAN defined in the first section of this notebook. The only thing that has changed is the normalization function. In this case we're using instance normalization.\n",
        "\n",
        "We use instance normalization instead of batch normalization to stabilize the learning process without introducing dependencies between the examples in a batch. Instance normalization normalizes the input across each channel for each example independently, which helps the Critic to focus on the structural content of each input image without being influenced by the variance across a batch of images."
      ],
      "metadata": {
        "id": "t6YrwfXepzRB"
      },
      "id": "t6YrwfXepzRB"
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, d_dim=64, img_size=64):\n",
        "        # \"d_dim\" is the output dimension of the first convolutional layer;\n",
        "        # that is, the number of filters/kernels you have in the first layer\n",
        "        # (3 inputs, RGB, and d_dim outputs)\n",
        "        super().__init__()\n",
        "\n",
        "        # Configuration parameters\n",
        "        kernel_size = 4\n",
        "        n = 4  # Number of conv layers. Only used for definition of fc_in\n",
        "        fc_in = int(d_dim * 2**(n-1) * (img_size/(2**n))**2)  # fc input dim\n",
        "        pad = 1\n",
        "        stride = 2\n",
        "        bias = False\n",
        "\n",
        "        # Helper function for convolutions\n",
        "        def conv(in_channels, out_channels):\n",
        "            return nn.Conv2d(in_channels=in_channels,\n",
        "                             out_channels=out_channels,\n",
        "                             kernel_size=kernel_size,\n",
        "                             stride=stride,\n",
        "                             padding=pad,\n",
        "                             bias=bias)\n",
        "\n",
        "        # Dense or fully connected layer\n",
        "        self.fc = nn.Linear(fc_in, 1)  # d_dim x 2^3 x (img_size / 2^4)^2 = 8192\n",
        "\n",
        "        # Convolutional layers (doubling the depth with each step)\n",
        "        self.conv1 = conv(in_channels=3,\n",
        "                          out_channels=d_dim)\n",
        "        self.conv2 = conv(in_channels=d_dim,\n",
        "                          out_channels=2*d_dim)\n",
        "        self.conv3 = conv(in_channels=2*d_dim,\n",
        "                          out_channels=4*d_dim)\n",
        "        self.conv4 = conv(in_channels=4*d_dim,\n",
        "                          out_channels=8*d_dim)\n",
        "\n",
        "        # Instance normalization layers.\n",
        "        self.inorm1 = nn.InstanceNorm2d(d_dim)\n",
        "        self.inorm2 = nn.InstanceNorm2d(2 * d_dim)\n",
        "        self.inorm3 = nn.InstanceNorm2d(4 * d_dim)\n",
        "        self.inorm4 = nn.InstanceNorm2d(8 * d_dim)\n",
        "\n",
        "        # Define a LeakyReLU activation function layer\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input shape: batch x 3 x img_size x img_size  -->  b x 3 x 64 x 64\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Out shape: b x d_dim x img_size/2 x img_size/2\n",
        "        out = self.leaky_relu(self.inorm1(self.conv1(x)))    # b x 64 x 32 x 32\n",
        "        out = self.leaky_relu(self.inorm2(self.conv2(out)))  # b x 128 x 16 x 16\n",
        "        out = self.leaky_relu(self.inorm3(self.conv3(out)))  # b x 256 x 8 x 8\n",
        "        out = self.leaky_relu(self.inorm4(self.conv4(out)))  # b x 512 x 4 x 4\n",
        "\n",
        "        # Flatten (b x 512 x 4 x 4  ==  b x 8192)\n",
        "        out = out.contiguous().view(batch_size, -1)  # b x 8192\n",
        "\n",
        "        # Final output layer without activation function\n",
        "        scores = self.fc(out)  # b x 1\n",
        "        return scores"
      ],
      "metadata": {
        "id": "HTFBcpZeY3s_"
      },
      "id": "HTFBcpZeY3s_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like the previous loss function, this one also takes only the tensor and a parameter indicating if it's from a real or fake image as inputs.\n",
        "\n",
        "Note that in this case the loss function, instead of being BCE, is directly the average of the input tensor, with a plus or minus sign depending on whether the images are fake or real."
      ],
      "metadata": {
        "id": "0bdsxgJf9lLx"
      },
      "id": "0bdsxgJf9lLx"
    },
    {
      "cell_type": "code",
      "source": [
        "def Wasserstein_loss_fcn(input_tensor, **kwargs):\n",
        "\n",
        "    # Extract label_type from **kwargs with defaults\n",
        "    label_type = kwargs.get(\"label_type\", \"real\")\n",
        "\n",
        "    # Ensure label_type is valid (real or fake)\n",
        "    label_type = \"real\" if label_type.lower() not in (\"real\", \"fake\") \\\n",
        "        else label_type.lower()\n",
        "\n",
        "    # Get sign of the losses\n",
        "    sign = -1 if label_type == 'real' else 1\n",
        "\n",
        "    return input_tensor.mean() * sign"
      ],
      "metadata": {
        "id": "mW6fYjNUY3vr"
      },
      "id": "mW6fYjNUY3vr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are about to define the penalty function, utilizing the same training algorithm as in our prior experiments. Could you investigate how this function was implemented during our training with the classic GAN setup? Specifically, where it was defined and the values it assumed are of interest.\n",
        "\n",
        "This penalty function, including a slight modification with the addition of a gamma multiplier, is outlined in the code found at the provided link, referencing the relevant paper. [link to the GitHub](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py)"
      ],
      "metadata": {
        "id": "fLgnYTRcvVHK"
      },
      "id": "fLgnYTRcvVHK"
    },
    {
      "cell_type": "code",
      "source": [
        "def penalty_fcn(real_img, fake_img, critic, gamma=10):\n",
        "    \"\"\"\n",
        "    Calculate gradient penalty\n",
        "    \"\"\"\n",
        "\n",
        "    # Interpolation between real and fake images\n",
        "\n",
        "    # Define alpha with shape bs x 1 x 1 x 1\n",
        "    alpha = torch.rand(real_img.shape[0], 1, 1, 1,\n",
        "                       device=DEVICE,\n",
        "                       requires_grad=True)\n",
        "\n",
        "    # Calculate interpolated images\n",
        "    # You can do it this way, that is easier to read, or with torch.lerp, that\n",
        "    # is faster: `mix_images = real_img * alpha + fake_img * (1 - alpha)`\n",
        "    mix_images = torch.lerp(fake_img, real_img, alpha)\n",
        "\n",
        "    # Get the critic prediction for the mixed images\n",
        "    mix_pred = critic(mix_images)\n",
        "\n",
        "    # Calculate the gradients of the output with respect to the input.\n",
        "    # I.e. the gradients of the prediction with respect to the mixed images.\n",
        "    gradients = torch.autograd.grad(\n",
        "        inputs=mix_images,\n",
        "        outputs=mix_pred,\n",
        "        grad_outputs=torch.ones_like(mix_pred),\n",
        "        retain_graph=True,\n",
        "        create_graph=True,\n",
        "        only_inputs=True)[0]\n",
        "\n",
        "    # Reshape gradients to calculate norm\n",
        "    gradients = gradients.view(len(gradients), -1)  # Flatten the gradients\n",
        "\n",
        "    # Use torch.norm to calculate the L2 norm of the gradients\n",
        "    gradient_norm = torch.norm(gradients, p=2, dim=1)\n",
        "\n",
        "    # Calculate the gradient penalty\n",
        "    gp = ((gradient_norm - 1) ** 2).mean()\n",
        "\n",
        "    return gp * gamma"
      ],
      "metadata": {
        "id": "IQnl1t8vY3yT"
      },
      "id": "IQnl1t8vY3yT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we define the optimizers."
      ],
      "metadata": {
        "id": "WX-7lPg47x4z"
      },
      "id": "WX-7lPg47x4z"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(SEED)\n",
        "G = Generator()\n",
        "C = Critic()\n",
        "\n",
        "# Generator optimizer\n",
        "\n",
        "# Parameters\n",
        "lr_g = 0.0002\n",
        "beta1 = 0.65\n",
        "beta2 = 0.999\n",
        "\n",
        "# Create optimizers for generator G\n",
        "g_optimizer = torch.optim.Adam(G.parameters(), lr_g, [beta1, beta2])\n",
        "\n",
        "\n",
        "# Critic optimizer\n",
        "\n",
        "# Parameters\n",
        "lr_d = 0.0002\n",
        "beta1 = 0.65\n",
        "beta2 = 0.999\n",
        "\n",
        "# Create optimizer for the discriminator D\n",
        "c_optimizer = torch.optim.Adam(C.parameters(), lr_d, [beta1, beta2])"
      ],
      "metadata": {
        "id": "mVSSb4IDY32L"
      },
      "id": "mVSSb4IDY32L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally we train again and see how it goes."
      ],
      "metadata": {
        "id": "zXSIY9kn9_ha"
      },
      "id": "zXSIY9kn9_ha"
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of epochs to train.\n",
        "n_epochs = 40\n",
        "\n",
        "# Note that with this configuration we don't save checkpoints\n",
        "\n",
        "config = {'n_epochs': n_epochs,\n",
        "          'dataloader': dataloader,\n",
        "          'discriminator': C,                  # The Critic\n",
        "          'generator': G,\n",
        "          'd_optimizer': c_optimizer,\n",
        "          'g_optimizer': g_optimizer,\n",
        "          'project_dir': PROJECT_DIR,\n",
        "          'loss_fcn': Wasserstein_loss_fcn,    # Wasserstein loss function\n",
        "          'show_step': 5,\n",
        "          'penalty_fcn': penalty_fcn,          # Gradient penalty\n",
        "          'crit_cycles': 5,\n",
        "          'save_step': 1000,\n",
        "          'save_starting': 1000,\n",
        "          }\n",
        "\n",
        "_ = train(config)"
      ],
      "metadata": {
        "id": "vlj452ISwG8b"
      },
      "id": "vlj452ISwG8b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}